---
tags: [machine-learning, machine-learning/neural-network]
---

# CLIP

[Contrastive Language-Image Pre-training](https://en.wikipedia.org/wiki/Contrastive_Language-Image_Pre-training) (CLIP) is a technic to train two [neural networks](/engineering/machine-learning/neural-network/neural-network.md), one for text and one for image. Both are encoded in the same latent space to find which images are correlated with which textual content.

A [self-supervised Learning](/engineering/machine-learning/learning-paradigms/self-supervised.md) strategy is used to associate the strength of the correlation between the text and the image.